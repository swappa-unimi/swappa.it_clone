Lezione 21 ottobre
------------------

in lab

http://www.dti.unimi.it/~ferrari/reti_neurali/toolbox_nn/

notare:

* l'apprendimento non si fa con la backpropagation, ma con il Levenberg-Marquardt
* aumentando il numero di epoche il tutto migliora
* aumentando il numero di neuroni non è detto migliori. Se lo fai un po' di volte magari in qlke caso esce meglio. Quindi non si può prevedere a priori come la rete si comporterà

Cliccando sul pulsante performance si ottiene il grafico degli errori. tutti gli errori. In particolare se l'errore di training è sotto quello di validazione potrebbe dire che la rete sta overfittando; oppure che l'insieme di validazione è troppo piccolo. 
Diminuendo il numero di neuroni si aumenta la capacità di generalizzazione della rete. 
Dal grafico delle performance scopriamo qual è un buon punto in cui fermare l'addestramento per evitare l'overfitting.

Pulsante regression. Se R è abbastanza vicino a 1 vuol dire che abbiamo una buona stima. Se i punti fuori dalla retta fossero finiti nel training avremmo avuto una ricostruzione peggiore.

Se forziamo ad utilizzare la backpropagation i risultati peggiorano in modo imbarazzante.

--

doll

dove non ci sono i dati la rete fa quello che vuole