<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
	<title>Swappa :: - LPS - Testing </title>
	<meta http-equiv='Content-Type' content='text/html; charset=ISO-8859-1' />
	<meta http-equiv='Content-Language' content='it' />
	<meta http-equiv='Content-Style-Type' content='text/css' />
	<meta http-equiv="imagetoolbar" content="no" />
	<meta name="robots" content="noarchive" />
	<meta name="verify-v1" content="W0mbMJBKN/iA23fcLw5UzRqXfwbh0SdGZ62YJgVCtqg=" />
	<meta name='description' content="Wiki creato da e per gli studenti del DTI dell'Università di Crema (Informatica, Sicurezza, DTI). Contiene appunti, riassunti, guide, esercizi, temi d'esame."  />
	<meta name='keywords' content="wiki, università, studenti, crema, appunti, riassunti, guide, informatica, sicurezza, esami, programmazione, recensioni, esercizi, progetti" />
	
  <link rel='icon' href='../pub/skins/sticWin/sticWinicon.gif.html' type='image/gif' />
  <link rel='SHORTCUT ICON' href='../pub/skins/sticWin/sticWinicon.gif.html' />
	<!--HeaderText--><style type='text/css'><!--
  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }
  code.escaped { white-space: nowrap; }
  .vspace { margin-top:1.33em; }
  .indent { margin-left:40px; }
  .outdent { margin-left:40px; text-indent:-40px; }
  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }
  a.createlink { text-decoration:none; position:relative; top:-0.5em;
    font-weight:bold; font-size:smaller; border-bottom:none; }
  img { border:0px; }
  
.progress-bar {
	display: block;
	background: transparent; 
	width: 520px;
	font-size: 1px; /* for IE */
	margin: 2px 0;
}

.progress-bar .pb1, .progress-bar .pb2, .progress-bar .pb3, .progress-bar .pb4 {
	display: block; 
	background: #fff; 
	border-left:  1px solid #999; 
	border-right: 1px solid #999;

	overflow: hidden; 
	height: 1px; 
}
.progress-bar .pb1 { margin: 0 4px; background: #999;}
.progress-bar .pb2 { margin: 0 2px; border-width: 0 2px; }
.progress-bar .pb3 { margin: 0 1px; }
.progress-bar .pb4 { height: 11px; padding: 0 3px; }
.progress-bar .pb5 { display: block; background: #eeeeef; overflow:hidden; }

.progress-bar .bar {
	display: block;
	background: #a5bbd8;
	height: 11px;
	padding: 0;
}
.editconflict { color:green; 
  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }

  table.markup { border:2px dotted #ccf; width:90%; }
  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }
  table.vert td.markup1 { border-bottom:1px solid #ccf; }
  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }
  table.markup caption { text-align:left; }
  div.faq p, div.faq pre { margin-left:2em; }
  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }
  div.faqtoc div.faq * { display:none; }
  div.faqtoc div.faq p.question 
    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }
  div.faqtoc div.faq p.question * { display:inline; }
   
    .frame 
      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }
    .lfloat { float:left; margin-right:0.5em; }
    .rfloat { float:right; margin-left:0.5em; }
a.varlink { text-decoration:none; }

--></style>
  <link rel='stylesheet' href='../pub/wsplus/wsplus.css' 
    type='text/css' />
  <!--[if IE]><style type='text/css' media='screen'>
    body { behavior:url('http://www.swappa.it/wiki/pub/wsplus/csshover.htc'); }
    .rollover * { visibility: visible; }
  </style><![endif]-->
<script type='text/javascript' src='../pub/syntaxlove/scripts/shCore.js'></script><script type='text/javascript' src='../pub/syntaxlove/scripts/shBrushCSharp.js'></script><script type='text/javascript' src='../pub/syntaxlove/scripts/shBrushCpp.js'></script><script type='text/javascript' src='../pub/syntaxlove/scripts/shBrushJava.js'></script><script type='text/javascript' src='../pub/syntaxlove/scripts/shBrushPerl.js'></script><script type='text/javascript' src='../pub/syntaxlove/scripts/shBrushPhp.js'></script><script type='text/javascript' src='../pub/syntaxlove/scripts/shBrushPython.js'></script><script type='text/javascript' src='../pub/syntaxlove/scripts/shBrushRuby.js'></script> <link type='text/css' rel='stylesheet' href='../pub/syntaxlove/css/shCore.css'/>
  <link type='text/css' rel='stylesheet' href='../pub/syntaxlove/css/shThemeDefault.css'/>
  <script type='text/javascript'>
  	SyntaxHighlighter.config.clipboardSwf = 'http://www.swappa.it/wiki/pub/syntaxlove/scripts/clipboard.swf';
  	SyntaxHighlighter.all();
  </script>  <meta name='robots' content='index,follow' />
  <link rel='stylesheet' href='../pub/skins/sticWin/sticWin.css' type='text/css' />
  
</head>

<body>
	<script type="text/javascript">
	  var _gaq = _gaq || [];
	  _gaq.push(['_setAccount', 'UA-5744461-3']);
	  _gaq.push(['_trackPageview']);

	  (function() {
	    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
	    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
	    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
	  })();
	</script>

	<!--PageHeaderFmt-->
	<table summary='Site: Header' id='siteheadtable' width='960'
	cellpadding='0' cellspacing='0' border='0' align='center'>
		<tr>
			<td colspan='2' id='navi'>
			<table width='420' cellpadding='0' cellspacing='0' align='right'>
			<tr>
				<td id='navi' width='300'>
					<!-- Inizio Codice Shinystat -->
					<script type="text/javascript" language="JavaScript" src="http://codice.shinystat.com/cgi-bin/getcod.cgi?USER=swappa"></script>
					<noscript>
					<a href="http://www.shinystat.com/it" target="_top">
					<img src="http://www.shinystat.com/cgi-bin/shinystat.cgi?USER=swappa" alt="Contatori visite gratuiti" border="0"></a>
					</noscript>
					<!-- Fine Codice Shinystat -->
				</td>
				<td id='riemp2'></td>
				<td id='navi' width='80'>
					<a rel="license" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/2.5/it/">
						<img alt="Creative Commons License" style="border-width:0" height="15" width="80" src="http://i.creativecommons.org/l/by-nc-sa/2.5/it/80x15.png" />
					</a>
				</td>
			
				<td id='riemp5'></td>
				
			</tr>
			</table>
			</td>	
		</tr>
		<tr>
			<td colspan='2' id='siteheadfind'>
				<a href='../Site.Search.html'
				title='Cerca nel sito' rel='nofollow'>cerca</a> 
				<form class='siteheadfind search' action='../index.html'>
				<input class='searchbox' type='text' name='q' value='' size='40' />
				<input class='searchbut' type='submit' value='vai' />
				<!-- version > beta53 -->
				<!-- <input type='hidden' name='n' value='Uni.LPSTesting' /><input type='hidden' name='action' value='search' /> -->
				<!-- version < beta53 -->
				<input type='hidden' name='n' value='Site.Search' />
				</form>
			</td>					
		</tr>
		<tr><td id='riemp1'></td></tr>
		<tr>
			<td id='pagetitle' width='560'>
				- LPS - Testing
			</td>
			<td id='siteheadcmds'>
			<table width='360' cellpadding='0' cellspacing='0' align='right'>
			<tr>
				<td id='siteheadmenu'>
					<a rel='nofollow' href='http://www.swappa.it/wiki/Uni/LPSTesting?action=edit' title='Modifica la pagina'>modifica</a>
				</td> 
				<td id='siteheadmenu'>
					<a  rel='nofollow' href='http://www.swappa.it/wiki/Uni/LPSTesting?action=diff' title='Ultime modifiche della pagina'>cronologia</a>
				</td>
				<td id='siteheadmenu'>
					<a  rel='nofollow' href='LPSTesting-action=print.html' title='Stampa la pagina'>stampa</a>
				</td>	
				<td id='riemp5'></td>
				<td id='log'>
					<a rel='nofollow' href='http://www.swappa.it/wiki/Uni/LPSTesting?action=login' title='Login'>login</a>
				</td>	
				<td id='log'>
					<a rel='nofollow' href='http://www.swappa.it/wiki/Uni/LPSTesting?action=logout' title='Logout'>logout</a>
				</td>	
				<td id='riemp2'></td>
			</tr>
			</table>
			</td>				
		</tr>  
		
	</table>
	<!--/PageHeaderFmt-->

	<table summary='Site: Main' id='contentmain' width='960'
	cellpadding='0' cellspacing='0' border='0' align='center'>
	<tbody>
		<tr>
			<td id='riemp2'></td>
			<!--PageLeftFmt-->
			<td id='mainsidebar'>
			
			<p class='sidehead'> Wiki
</p><ul><li><a class='wikilink' href='../Main/HomePage.html'>HomePage</a>
</li><li><a class='wikilink' href='http://www.swappa.it/wiki/Main/Forum'>Forum</a>
</li><li><a target='_blank'  class='urllink' href='http://www.swappa.it/gallery/main.php' title='' rel='nofollow'>Galleria</a>
</li></ul><p class='vspace sidehead'> UniCrema
</p><ul><li><a class='wikilink' href='../Category/UniCrema.html'>Tutte le materie</a>
</li><li><a class='wikilink' href='../Category/Docenti.html'>Docenti</a>
</li><li><a class='wikilink' href='Progetti.html'>Progetti</a>
</li></ul><p><br clear='all' />
</p>
<p class='vspace sidehead'> Materie per semestre
</p><ul><li><a class='wikilink' href='../Category/PrimoSemestre.html'>Primo semestre</a>
</li><li><a class='wikilink' href='../Category/SecondoSemestre.html'>Secondo semestre</a>
</li></ul><p class='vspace sidehead'> Materie per anno
</p><ul><li><a class='wikilink' href='../Category/PrimoAnno.html'>Primo anno</a>
</li><li><a class='wikilink' href='../Category/SecondoAnno.html'>Secondo anno</a>
</li><li><a class='wikilink' href='../Category/TerzoAnno.html'>Terzo anno</a>
</li><li><a class='wikilink' href='../Category/ComplementariOMagistrale.html'>Complem. / magistrale</a>
</li></ul><p class='vspace sidehead'> Materie per laurea
</p><ul><li><a class='wikilink' href='../Category/Informatica.html'>Informatica</a>
</li><li><a class='wikilink' href='../Category/Sicurezza.html'>Sicurezza</a>
</li><li><a class='wikilink' href='../Category/TS.html'>TS</a>
</li></ul><p><br clear='all' />
</p>
<p class='vspace sidehead'> Help
</p><ul><li><a class='wikilink' href='http://www.swappa.it/wiki/Category/GuideInterneWiki'>Guide interne</a>
</li><li><a class='wikilink' href='../Main/FAQ.html'>FAQ</a>
</li></ul><div class='vspace'></div>

			
				<div id='sidebarbottom'>
					<span style='white-space:nowrap;'>
						<a  rel='nofollow' href='http://validator.w3.org/check/referer'
						title='Validate XHTML'>XHTML</a>
					</span>
				</div>
			</td>
			<!--/PageLeftFmt-->
			<td id='riemp1'></td>
			<td id='mainsitetext'>
				<!-- table to work around an NS4 bug -->
				<table summary='Content Body' id='sitebody'
				cellpadding='0' cellspacing='0' border='0'> 
					<tr>
						<td>
							<!--PageTitleFmt-->
							<div id='sitepagetitle'
							title='- LPS - Testing was last modified on January 20, 2010, at 09:29 AM'>
							<span style='display:none;'>- LPS - Testing</span>	
							</div>
							<!--PageText-->
<div id='wikitext'>
<p>
</p><pre  style='text-align: center; background-color: #ffe4c4; border: 2px solid #cccccc; font-size: 13pt; padding: 5px;'> <strong>:: LPS - Testing ::</strong>
</pre><div class='vspace'></div><h2>Perché il testing</h2>
<p>Il sw al 100 % senza difetti non esiste. Anzi, in generale l'industria del software è l'unica che permette la vendita di prodotti <em>con</em> difetti, e poi chiede soldi per fissare i bug.
</p>
<p class='vspace'>Nell'ingegeneria classica è prevista una fase di testing, ma quando si cerca di applicare il concetto all'ingegneria del software ci sono dei problemi in cui scontrarsi:
</p><ul><li>nell'ingegneria classica, si devono testare certi precisi punti, predeterminati, o tutt'al più il loro intorno -&gt; nell'ignegneria del software se qualcosa funziona correttamente in un punto del dominio, non è affatto detto che nel suo intorno lo faccia ancora
</li><li>un ponte, per fare un esempio di ingegneria edile, deve resistere ad un certo carico. Ma che cosa vuol dire per un software essere resistente? Questo dipende dal tipo di software e dal suo utilizzo: non è affatto una cosa generica.
</li></ul><div class='vspace'></div><h3>Limiti del testing</h3>
<p>Cominciamo subito a dire che  il testing, e tutto ciò che studieremo in futuro al riguardo, ha questi limiti:
</p><ul><li>va bene per <em>trovare</em> i bug
</li><li>ma <em>non va bene</em> per provare l<em>'assenza</em> dei bug
</li></ul><p>Questo vuol dire che probabilmente facendo testing riuscirò a trovare dei bug, ma non esiste nessun testing che, alla fine della sua esecuzione, mi dirà con certezza che il mio software sia corretto.
</p>
<p class='vspace'>Più avanti saranno espressi i teoremi che dimostrano ciò.
</p>
<div class='vspace'></div><h3>Linee guida generali che si richiedono ad una procedura di testing</h3>
<p>Il testing dovrebbe avere certe caratteristiche. Accanto ad ogni caratteristica c'è scritto se essa viene osservata o no al giorno d'oggi:
</p><ul><li>automatizzato -&gt; <strong>sì</strong>
</li><li>riguardare tutte le fasi dello sviluppo, e non solo la stesura del codice (eg: specifica, progettazione, manutenzione etc.) -&gt; <strong>no</strong>
</li><li>essere esteso a tutti i componenti di un sistema -&gt; <strong>sì</strong>
</li><li>essere pianificato -&gt; <strong>solo per certi sw</strong>
</li><li>seguire degli standard -&gt; <strong>no</strong>
</li></ul><div class='vspace'></div><h3>Economia del testing</h3>
<p>Vanno dette subito alcune cose:
</p><ul><li>all'inizio di una fase di testing, si trovano tanti bug. Poi il tempo passa e se ne trovano sempre di meno, ma ovviamente non sappiamo se sono finiti o no
</li><li>la maggior parte dei bug sono banali, mentre quelli critici sono pochi: assieme al punto precedente, ne consegue che si trova la maggior parte dei bug banali all'inizio, mentre quelli critici, SE si trovano, li si trova dopo tanto tempo.
</li></ul><p class='vspace'>Ci sono delle procedure di <strong>analisi statica</strong> che permettono di verificare la correttezza di un sw, ma sono molto molto costose (in pratica trasformano un programma in una formula logica, la cui verità è più o meno facilmente dimostrabile).
</p>
<p class='vspace'>In genere, visto che la fase di testing è costosa, occorre sapere innanzitutto quanto costa proseguire, e confrontare ciò con il costo della distribuzione di un software difettoso: <em>costo sw difettato = costo del danno che produce * probabilità che il danno accada</em>.<br />Quindi, se la prob che un certo danno accada è bassa, ci si può permettere di dedicare le proprie forze a qualcos'altro.
</p>
<p class='vspace'>Problema: <strong>chi</strong> dice che un danno costa poco, e che la sua probabilità di accadere è bassa? Occorrono dei <strong>criteri</strong> formali per stabilire ciò, non basta l'intuizione...
</p>
<div class='vspace'></div><h2>Terminologia</h2>
<ul><li><strong>malfunzionamento (failure)</strong>: il programma non funziona in modo corretto
</li><li><strong>difetto (fault, bug)</strong>: un elemento del codice sorgente non corrisponde alle aspettative
</li><li><strong>errore</strong>: fattore umano che ha causato il difetto
</li></ul><p class='vspace'>Ho quindi una sequenza di cause: errore -&gt; difetto -&gt; malfunzionamento. Vuol dire che c'è un errore umano che porta all'implementazione di codice buggato, e la conseguenza è che il programma non funziona come dovrebbe.
</p>
<p class='vspace'>Il <strong>testing</strong> consiste nell'eseguire il programma in certi casi di test, opportunamente studiati, così che ci si possa accorgere di eventuali malfunzionamenti, e scoprire quali siano i difetti che li causano. La colpa invece sarà sempre degli altri.
</p>
<p class='vspace'>Il <strong>debugging</strong> è invece il correggere i difetti dopo che si sono scoperti errori. Quindi debugging e testing non sono sinonimi.
</p>
<div class='vspace'></div><h2>Tipi di testing</h2>
<p>I testing si possono classificare in 3 modi:
</p><ul><li>livello
</li><li>aspetti
</li><li>accessibilità
</li></ul><p>Una procedura di testing quindi, a seconda di come viene classificata secondo queste 3 categorie, sarà posizionata in un certo punto del cubo ideale con queste 3 categorie sugli assi,
</p>
<div class='vspace'></div><h3>Livelli</h3>
<p>I <strong>livelli</strong> sono i test rispetto a quello che dovrebbe essere il programma, secondo diversi tipi di requisiti. Requisiti diversi portano a test diversi:
</p><ul><li>requisiti dell'utente -&gt; acceptance testing
</li><li>specifica -&gt; conformance testing
</li><li>progetto -&gt; integration testing
</li><li>codice -&gt; unit testing
</li><li>retrocompatibilità -&gt; regression testing
</li></ul><p class='vspace'>Ora li vediamo un po' tutti, ma quello che faremo noi sarà esclusivamente <strong>unit testing</strong>.
</p>
<div class='vspace'></div><h4>Unit testing</h4>
<p>Prendo una unità di codice, che sarebbe una classe, la quale va a testare i singoli metodi delle classi che ho implementato.
</p>
<p class='vspace'>Come faccio a testarli? Creo del codice, chiamato <strong>test driver</strong>, il quale chiama il metodo sotto esame con certi parametri, e verifica che il metodo si comporti in modo conforme (conforme a che cosa? Alle specifiche? Non sempre -&gt; poi vedremo).
</p>
<p class='vspace'>Che cosa succede se il metodo che sto esaminando a sua volta fa appello ad altri metodi? La soluzione è creare <strong>test stub</strong>, cioè abbozzi di test. Se nel metodo <strong>A</strong> ho la chiamata al metodo <strong>B</strong>, e voglio evitare di impegolarmi nell'analisi del metodo <strong>B</strong>, allora sostituisco le chiamate al metodo <strong>B</strong> con degli abbozzi, che restituiscono un valore a me noto. In questo modo sono in grado di isolare i malfunzionamenti presenti in <strong>A</strong> e dovuti al codice di <strong>A</strong>, rispetto ai malfunzionamenti presenti in <strong>A</strong> ma dovuti al codice che <strong>A</strong> chiama, come ad esempio <strong>B</strong>.
</p>
<div class='vspace'></div><h4>Test d'integrazione</h4>
<p>Dopo il test sulle singole unità, occorre testare che tutte insieme vadano d'accordo. Non è affatto così scontato. In generale, c'è la nozione di <strong>importazione</strong> ed <strong>esportazione</strong>. Un metodo importa qualcosa ed esporta qualcos'altro.
</p><ul><li>i valori in uscita di un metodo, che fanno da ingresso per un altro metodo, sono compatibili? Di solito ci pensa il compilatore (e non dovrebbe farlo, a mio avviso).
</li><li>i dominî di import e export sono gli stessi?
</li><li>I dati importati ed esportati sono rappresentati allo stesso modo?
</li></ul><p class='vspace'>È eclatante il caso di una sonda Pioneer che crashò perché c'era del codice che trattava una variabile in sistema metrico decimale, e dell'altro codice ad esso collegato che gli passava la variabile, ma da lui considerata secondo il sistema imperiale!
</p>
<p class='vspace'>Le tecniche per fare test di integrazione sono le seguenti:
</p><ul><li><strong>top-down</strong>: scrivo il test driver di un metodo, e stubbo tutte le sotto chiamate. Poi faccio lo stesso per ogni sottochiamata etc. etc. fino ad arrivare in fondo
</li><li><strong>bottom-up</strong>: faccio il test driver di un metodo base, e man mano faccio il test drive di tutti i metodi che si rifanno a lui, e poi salgo
</li><li><strong>big-bang</strong>: faccio il test simultaneo di tutte le componenti (per risparmiare tempo)
</li></ul><div class='vspace'></div><h4>Regression testing</h4>
<p>È il livello più alto di testing. Voglio che il mio programma lavora correttamente anche rispetto alle sue versioni precedenti. Siccome i costi di sviluppo incidono per 1/3 sulle spese totali di produzione di un sw, e quelli di manutenzione per i 2/3, se ogni volta che faccio qualcosa devo ricominciare a manutenere da capo, allora è grama.
</p>
<p class='vspace'>Quindi, quando modifico un programma, devo stare attento:
</p><ul><li>devo eliminare i bug che sono stati segnalati nell'ultima versione
</li><li>non devo introdurne di nuovi, per quanto possibile
</li><li>devo poter riutilizzare tranquillamente i vecchi casi di test
</li><li>riutilizzando i vecchi casi di test devo accertarmi che il comportamento sia rimasto uguale
</li></ul><div class='vspace'></div><h3>Aspetti</h3>
<p>Degli aspetti non si fa niente qui. Infatti si tratta delle stesse cose che abbiamo visto ad inizio corso relative alle proprietà del software, in particolare quello sicuro.
</p>
<div class='vspace'></div><h3>Accessibilità</h3>
<p>Stiamo parlando del tipo di accesso che i tester hanno sul codice:
</p><ul><li><strong>black-box</strong>: non ho accesso al codice, e tratto quindi il sw come una scatola chiusa, avendo in mano solo la specifica
</li><li><strong>white-box</strong>: ho accesso al codice
</li></ul><p class='vspace'>La differenza tra queste due metodologie non è affatto banale e scontata. Spesso i metodi white-box si chiamano <strong>test basato sul programma</strong>, mentre quelli black-box <strong>test basato sulle specifiche</strong>. C'è anche la grey-box, che è una via di mezzo, ma vabbeh. Adesso vediamo nello specifico che cosa cambia tra le due scatole.
</p>
<div class='vspace'></div><h4>White-box</h4>
<p>Con il testing white-box, si derivano i casi di test direttamente dal programma. Controllo il programma mentre questo viene eseguito, vedo quanta percentuale del programma ho coperto, e vedo come si comporta.
</p>
<p class='vspace'>La <strong>copertura</strong> del programma, ovvero quanta parte di esso è stata compresa nel mio test, è un criterio oggettivo sulla validità del mio testing.
</p>
<p class='vspace'>Ma nel sistema white-box c'è qualcosa di insitamente <strong>non oggettivo</strong>, che è la derivazione dei casi di test dal codice sorgente. In pratica l'analizzatore guarda il codice, stabilisce come <strong>secondo lui</strong> dovrebbe funzionare, e ne deriva i test.
</p>
<div class='vspace'></div><h4>Black-box</h4>
<p>In questo tipo di testing si ignora la struttura del programma, ma guardo i soli requisiti. Accedo al programma solo dalla sua interfaccia esterna, e non guardo come è fatto dentro.
</p>
<p class='vspace'>Guardando e analizzando la specifica, si ricavano i test, e si controlla che il programma sia conforme alla specifica. Si vede subito che si tratta di un procedimento <strong>oggettivo</strong>.
</p>
<div class='vspace'></div><h4>Quale scelgo?</h4>
<p>Perché, ci si chiederà, esiste ancora il white-box, che è un po' aleatorio? Vediamo un po' i pro ed i contro di ciascuna delle due metodologie.
</p>
<p class='vspace'>Il <strong>white-box</strong> ha queste caratteristiche:
</p><ul><li>pro: il codice è un'importante fonte di informazioni
</li><li>contro
<ul><li>è impossibile trovare errori di omissione. Se qualcuno <em>non</em> ha scritto del codice, è anche impossibile testarlo, e se non ho la specifica non ho nemmeno idea che stia mancando...
</li><li>non fornisce <strong>test oracles</strong>, cioè oracoli che mi dicono se il test ha evidenziato un malfunzionamento o no. In pratica, il criterio per stabilire se un test è negativo o positivo è quello che passa per la mente al tester in quel momento
</li></ul></li></ul><p class='vspace'>Il <strong>black-box</strong> ha queste altre caratteristiche:
</p><ul><li>pro: ha l'oracolo, che è la specifica. Se il programma rispetta la specifica, è giusto. Se no, no
</li><li>contro:
<ul><li>non sempre le specifiche sono disponibili
</li><li>quando sono disponibili, non sempre le specifiche sono scritte in modo formale. Se non sono scritte in modo formale, non posso ricavare in automatico i casi di test
</li><li>è molto più faticoso da tirare in piedi.
</li></ul></li></ul><div class='vspace'></div><h2>Testing basato sui programmi</h2>
<p>Qui vediamo in dettaglio il white-box testing. Il black-box lo trascuriamo, perché troppo complesso. Si affronta in altri corsi.
</p>
<p class='vspace'>Dobbiamo arrivare alla definizione del <strong>grafo di flusso</strong> di un programma, per poi affrontre tutto il resto.
</p>
<div class='vspace'></div><h3>Definizioni</h3>
<p>Il programma <strong>P</strong> lo dobbiamo considerare come una <strong>funzione</strong> che va da un dominio ad un codominio: <strong>P:D-&gt;R</strong>. È una funzione <strong>parziale</strong>, perché non è detto che sia definita su tutti gli elementi del dominio.
</p>
<p class='vspace'>Il <strong>predicato OK</strong> è in realtà un predicato multiforme:
</p><ul><li><strong>OK(P,d)</strong> è vero se il programma <strong>P</strong> è corretto per l'input <strong>d</strong> appartenente al dominio <strong>D</strong>
</li><li><strong>OK(P)</strong> è vero se il programma <strong>P</strong> è corretto per ogni <strong>d</strong> appartenente a <strong>D</strong>
</li></ul><p class='vspace'>Tramite il predicato OK posso dare le definizioni di failure, bug ed errore:
</p><ul><li><strong>failure</strong> = <strong>~OK(P,d)</strong>
</li><li><strong>bug (fault)</strong> = P è stato implementato nel programma P'. Se P != P', allora la diversità è il bug. In altre parole, esiste un <strong>d</strong> appartenente a <strong>D</strong> tale che OK(P,d) != OK(P',d)
</li><li><strong>errore</strong> = eh no, questo non possiamo definirlo in termini di OK, perché è la causa umana di tutto:)
</li></ul><p class='vspace'>Il <strong>caso di test</strong> è semplicemente un elemento del dominio del programma. Un <strong>test set (insieme di test)</strong>, indicato con <strong>T</strong>, è un sottinsieme finito del dominio del programma.
</p>
<p class='vspace'>Il predicato OK si trasforma ancora e diventa <strong>OK(P,T)</strong>, che restituisce <strong>true</strong> se il programma è corretto per ogni elemento <strong>t</strong> appartenente al test set <strong>T</strong>.
</p>
<p class='vspace'>Se il test set è negativo, vuol dire che non sono stati riscontrati errori nel programma. Se è positivo è il contrario.
</p>
<div class='vspace'></div><h3>Test ideale</h3>
<p>L'ideale sarebbe avere il <strong>test ideale</strong>, ovvero prendere un sottinsieme finito del dominio D, controllare il programma contro questo sottinsieme, e poter dedurre per via di qualche magia che, se il programma è corretto per questo sottinsieme, allora è corretto per tutto il dominio.
</p>
<p class='vspace'>Purtroppo tutto ciò è vana speranza: in generale il test ideale non esiste. C'è per piccole funzioni, ma non c'è nessun algoritmo che dato un programma ci produca il test ideale.
</p>
<p class='vspace'>C'è anche un'altra cosa da notare: se T coincide con D, allora sì che il test ideale esiste: è tutto il dominio stesso! È ovvio che se il programma funziona per tutto il dominio, allora è corretto. Ma il problema è che spesso (spessissimo) il dominio, per quanto finito, è gigantesco e quindi non è possibile fare un <strong>test esaustivo</strong>.
</p>
<p class='vspace'>Per fare un esempio, se ho una funzione che prende per argomento due numeri interi, il test esaustivo dovrebbe verificare tutte le coppie possibili di interi passate alla funzione. Se ho 2<sup>32</sup> possibili interi sul mio computer, allora il test esaustivo dovrebbe eseguire 2<sup>32</sup> * 2<sup>32</sup> controlli! In qualche migliaio di anni ce la caviamo.
</p>
<p class='vspace'>Se poi il dominio è infinito, allora non c'è speranza di fare un test esaustivo.
</p>
<div class='vspace'></div><h3>Test di adeguatezza</h3>
<p>Ma noi persistiamo nell'utopia di volere un test set che funzioni bene. Per fare ciò, introduciamo il <strong>test di adeguatezza</strong>, detto anche <strong>criterio di adeguatezza</strong> o, più semplicemente, <strong>criterio</strong>.
</p>
<p class='vspace'>Il criterio, nella sua forma iniziale, va visto come un predicato che, dato il programma <strong>P</strong>, la specifica <strong>S</strong> e il test <strong>T</strong>, mi dice se quel test <strong>T</strong> è adatto a verificare la correttezza di <strong>P</strong> rispetto ad <strong>S</strong>: <strong>C:P*S*T -&gt; {true, false}</strong>.
</p>
<p class='vspace'>Però, se ho un test basato sul programma, la specifica <strong>S</strong> non ce l'ho. E se ho un test basato sulla specifica, il programma <strong>P</strong> non ce l'ho:
</p><ul><li>white-box =&gt; C: P*T -&gt;  {true, false}
</li><li>black-box =&gt; C: S*T -&gt;  {true, false}
</li></ul><p class='vspace'>Ma noi siamo più furbi, e allora cambiamo le carte in tavola. In pratica, vogliamo far diventare questo predicato una funzione <strong>generatrice di test set</strong>. In altre parole, voglio avere una funzione <strong>C: S*P -&gt; T</strong>, cioè data la specifica <strong>S</strong> ed il programma <strong>P</strong>, voglio che la funzione mi generi automaticamente un test set <strong>T</strong> adatto a verificare la correttezza del mio programma.
</p>
<div class='vspace'></div><h4>Proprietà del criterio</h4>
<p>Un criterio può avere queste proprietà:
</p><ul><li><strong>affidabile</strong> = un criterio è affidabile se, per ogni coppia di test set T1 e T2, un malfunzionamento rilevato da T1 viene rilevato anche da T2. In pratica sto chiedendo al criterio di essere coerente con tutti i test set che genera: tutti devono trovare (o non trovare) lo stesso errore, non sono ammessi comportamenti incoerenti tra due test set diversi generati dallo stesso criterio affidabile
</li><li><strong>valido</strong> = un criterio è valido se, tra tutti i test set che genera, ce n'è almeno uno che rileva un determinato malfunzionamento
</li><li><strong>ideale</strong> = un criterio è ideale se è allo stesso tempo valido ed affidabile
</li></ul><p class='vspace'>Per chiarire la differenza tra queste proprietà, vediamo un esempietto:
</p>
<div class='vspace'></div><pre class='escaped'>
int raddoppia (int x) {
	return x*x;
}
</pre>
<p class='vspace'>Questo codice è palesemente errato, perché la specifica che posso <strong>dedurre</strong> dal nome della funzione è che quella funzione debba restituire il doppio del parametro passato. Attenzione: siccome sto facendo white-box, non ho sottomano nessuna specifica, quindi la <strong>deduco</strong> dal nome della funzione, o dal contesto o quello che è. Se deduco male, di conseguenza creo anche dei test set poco utili.
</p>
<p class='vspace'>Tornando a noi, supponiamo di avere il criterio <strong>C</strong> che ci dà dei test set, e che i test set siano un insieme di valori che diamo in pasto alla funzione:
</p><ul><li>se <strong>C</strong> ci dice di testare i valori <strong>{0,1}</strong>, allora il criterio è affidabile. Infatti, sia che gli passi 0 che 1, il comportamento è corretto ed è coerente tra i due elementi
</li><li>se <strong>C</strong> mi dice di testare i valori <strong>{2,3}</strong> è valido perché in almeno uno dei casi (quando testo il 3) scopro il malfunzionamento
</li><li>se <strong>C</strong> mi dice di testare i valori <strong>{4,5,6}</strong> è affidabile, perché tutti i valori di test mi danno lo stesso comportamento, cioè l'errore. È valido perché scopre l'errore. È quindi ideale.
</li></ul><p class='vspace'>Quindi, ideale vuol dire che
</p><ul><li>scopro il difetto (valido)
</li><li>scopro sempre il difetto (affidabile)
</li></ul><div class='vspace'></div><h3>Teorema di Goodenough e Gerhart</h3>
<p>Il teorema dice che, se il mio criterio <strong>C</strong> è ideale. e il test sul programma <strong>P</strong> con i test set generati da <strong>C</strong> è negativo, allora il programma è corretto.
</p>
<div class='vspace'></div><h3>Teorema di Howden</h3>
<p>Non esiste un algoritmo che, dato un programma <strong>P</strong>, sia capace di generare un test ideale finito.
</p>
<p class='vspace'><strong>Dijkstra</strong> aggiunge che il test di un programma può rilevare quindi la presenza di un malfunzionamento, ma non può mai mostrarne l'assenza, proprio perché non è possibile avere un test ideale finito (a parte i casi banali...).
</p>
<div class='vspace'></div><h2>Testing strutturale dei programmi</h2>
<p>Adesso che abbiamo idea di che cosa fare, il nostro obiettivo è avere un sistema un po' meno "sentimentale" e più algoritmico per ricavare i criterî. Non è sempre immediato vedere quello che va verificato. Con il <strong>flusso di controllo del programma</strong>, sono in grado di valutare la copertura di un test.
</p>
<div class='vspace'></div><h3>Il flusso di controllo</h3>
<p>Non è nient'altro che il classico diagramma di flusso:
</p><ul><li>gli ovali sono le istruzioni
</li><li>i rombi le condizioni, e hanno un ramo d'uscita <strong>true</strong> ed un ramo <strong>false</strong>
</li></ul><p class='vspace'>Da notare che NON si scrivono come istruzioni (cioè negli ovali) le dichiarazioni delle variabili. Se da qualche parte ho <strong>int x;</strong> non lo metto nelle istruzioni. Ocio però che se ho <strong>int x = 10;</strong> allora sì, perché ho anche l'assegnamento, e l'assegnamento è un'istruzione e va messa negli ovali.
</p>
<div class='vspace'></div><h4>If... else...</h4>
<div><img src='../uploads/Uni/LPSif.png' alt='' title='' /></div>
<p class='vspace'>È abbastanza semplice: se la condizione è vera, vado da una parte, se no vado dall'altra.
</p>
<div class='vspace'></div><h4>While</h4>
<div><img src='../uploads/Uni/LPSwhile.png' alt='' title='' /></div>
<p>Il <strong>while</strong> è anch'esso una condizione, e notate che se essa non è soddisfatta si ritorna subito al controllo della condizione
</p>
<p class='vspace'>Il <strong>do...while</strong> è una variazione del <strong>while</strong>, in cui si ha la certezza che il codice viene eseguito almeno una volta:
</p><div><img src='../uploads/Uni/LPSdowhile.png' alt='' title='' /></div>
<div class='vspace'></div><h4>For</h4>
<p>Il ciclo <strong>for</strong> va visto come un'implementazione semplificata del ciclo <strong>while</strong>, come infatti è. Lo schemino dà un'idea della sua rappresentazione in diagrammi di flusso.
</p>
<div class='vspace'></div><div><img src='../uploads/Uni/LPSfor.png' alt='' title='' /></div>
<div class='vspace'></div><h2>Criteri di copertura</h2>
<p>Ci sono diversi criteri di copertura, cioè funzioni che ci dicono come creare i test set in modo da coprire il nostro programma.
</p>
<div class='vspace'></div><h3>Criterio di copertura delle istruzioni</h3>
<p>Qui lo scopo è coprire tutte le istruzioni. Il criterio dice che un test set T è adeguato a testare un programma P, secondo il criterio di copertura delle istruzioni, se per ogni istruzione <em>s</em> appartenente a <em>P</em> esiste un <em>t</em> appartenente a <em>T</em> che la esegue. Vogliamo in altre parole che ogni istruzione venga eseguita almeno una volta.
</p>
<div class='vspace'></div><div><span class='frame lfloat'><img src='../uploads/Uni/LPScopertura1.png' alt='' title='' /></span></div>
<p class='vspace'>In questo esempio, c'è una condizione, ma per eseguire almeno una volta tutte le istruzioni è possibile fare un giro solo.
</p>
<p class='vspace'><br clear='all' />
</p>
<p class='vspace'>Per <strong>valutare</strong> il criterio di copertura, si usa la <strong>fault detection capability</strong>, ovvero 
</p><ul><li>quali errori può trovare
</li><li>quali errori garantisce di trovare
</li></ul><p>La copertura delle istruzioni passa per tutte le istruzioni, e quindi trova tutte le istruzioni errate, ma non è per niente detto che trovi anche gli errori nelle decisioni.
</p>
<p class='vspace'>Per <strong>misurare</strong> quanto del mio programma ho coperto, semplicemente faccio <strong>numero di statement eseguiti / numero di statement eseguibili</strong> e ho un numero compreso tra 0 e 1 che mi dà la misura della mia copertura.
</p>
<div class='vspace'></div><h3>Branch coverage</h3>
<p>Qui, l'obiettivo non è eseguire tutte le istruzioni, bensì percorrere tutti i cammini. Voglio che un cammino sia percorso almeno una volta da un test.
</p>
<p class='vspace'>Ovviamente, rispetto alla copertura delle istruzioni, si hanno dei test set differenti. Prendendo lo stesso esempio di prima, per avere statement coverage bastava un test set solo, ma se voglio fare branch coverage me ne servono almeno due, perché dalla condizione si diramano appunto due cammini diversi. Si può anche affermare che il branch covering è più forte rispetto allo statement coverage, perché un test set che soddisfa il branch, soddisferà anche lo statement, ma non accade il viceversa.
</p>
<p class='vspace'>La misura della copertura è data da <strong>numero cammini eseguiti / numero cammini eseguibili</strong>.
</p>
<div class='vspace'></div><h3>Come generare i casi di test</h3>
<p>Si può procedere in un modo abbastanza meccanico, per generare i casi di test. Ecco i passaggi:
</p><ol><li>si fa il grafo di flusso
</li><li>si trovano i percorsi che vanno eseguiti
</li><li>per ognuno di questi percorsi, cerco i valori di input che fanno sì che il programma li percorra
</li><li>ottimizzo eliminando i test non necessari
</li></ol><p class='vspace'>Per cercare i valori di input, si può fare tutto a mente, ragionandoci sopra, oppure seguire questa scaletta:
</p><ol><li>si va alla fine del cammino in esame
</li><li>si procede all'indietro, saltando tutte le istruzioni di assegnamento fino alla prima condizione che si trova (cioè l'ultima del cammino), e si tiene nota del valore che la condizione vuole per eseguire il cammino
</li><li>continuando a ritroso, per ogni assegnamento sulle variabili trovate nelle condizioni già viste, si modifica il valore della variabile
</li></ol><div class='vspace'></div><h3>Copertura delle decisioni e delle condizioni</h3>
<p>Si tratta di altri due criteri, e per introdurli occorre stabilire la differenza tra una decisione ed una condizione.
</p>
<p class='vspace'>La <strong>decisione</strong> è tutto ciò che è contenuto nella guardia dell<strong>'if</strong>. Se ho una riga come
</p>
<div class='vspace'></div><pre class='escaped'>
if ((gatto &gt; 20) &amp;&amp; (cane &lt; 30)) { ...}
</pre>
<p class='vspace'>allora la parte <strong>((gatto &gt; 20) &amp;&amp; (cane &lt; 30))</strong> è la mia decisione.
</p>
<p class='vspace'>Invece, nella stessa riga ci sono due <strong>condizioni</strong>: <strong>(gatto &gt; 20)</strong> e <strong>(cane &lt; 30)</strong>.
</p>
<div class='vspace'></div><h4>Copertura delle decisioni</h4>
<p>Un test set è adeguato per coprire il programma <strong>P</strong> secondo il criterio di copertura delle decisioni, se per ogni decisione in <strong>P</strong> esiste:
</p><ul><li>un caso di T che la prende
</li><li>un caso di T che <em>non</em> la prende
</li></ul><p class='vspace'>È in pratica equivalente a fare il branch covering, perché i branch vengono appunto generati dalle condizioni.
</p>
<div class='vspace'></div><h4>Copertura delle condizioni</h4>
<p>Per ogni condizione, deve esistere un caso di test che la rende vera, ed un caso di test che la rende falsa.
</p>
<p class='vspace'>Prendiamo ad esempio la decisione <strong>(x &gt; 0) OR (y &gt; 0)</strong>. Le condizioni sono due, quindi <em>in teoria</em> devo avere:
</p><ol><li>un caso di test che renda vera <strong>x &gt; 0</strong>
</li><li>un caso di test che renda falsa <strong>x &gt; 0</strong>
</li><li>un caso di test che renda vera <strong>y &gt; 0</strong>
</li><li>un caso di test che renda falsa <strong>y &gt; 0</strong>
</li></ol><p class='vspace'>Il punto è che il primo ed il terzo caso di test portano allo stesso cammino (vedi sotto per MCC). Quindi, in realtà a me bastano 2 cammini:
</p><ul><li>(x = 1, y = -1)
</li><li>(x = -1, y = 1)
</li></ul><p class='vspace'>Infatti, il primo caso di test rende vera la prima e falsa la seconda, mentre il secondo caso di test rende falsa la prima e vera la seconda. Data la definizione di copertura delle condizioni, il criterio è soddisfatto.
</p>
<div class='vspace'></div><h3>MCC e MCDC</h3>
<p>Ci sono altri due criteri:
</p><ul><li>MCC = Multiple Condition Coverage
</li><li>MCDC = Modified Condition/Decision Coverage
</li></ul><div class='vspace'></div><h3>MCC</h3>
<p>Lo scopo dell'MCC è quello di testare ogni possibile combinazione dei valori di verità delle condizioni in ogni decisione.
</p>
<p class='vspace'>Nell'esempio di sopra, avrei dovuto semplicemente evitare di semplificare, ottenendo così 4 differenti casi di test. Poi nella realtà accade che i casi di test si sovrappongano, e che quindi si possano ridurre di numero.
</p>
<p class='vspace'>Questa faccenda è chiamata <strong>short circuit evaluation</strong>, ed è usata dai compilatori per creare codice più velocemente eseguibile.
</p>
<p class='vspace'>Prendiamo questa decisione: <strong>a OR b or c</strong>. In teoria ho 3 condizioni (<strong>a</strong>, <strong>b</strong> e <strong>c</strong>), e quindi 2'3^' righe nella tavola di verità. Ma il compilatore è furbo: siccome per rendere vero un OR basta che una sola delle sue condizioni sia vera, non appena ne trova una vera smette di valutare quelle successive, e decide che la condizione è vera.<br />Allo stesso modo, in <strong>a AND b AND c</strong>, il compilatore sa che basta che una sola di queste condizioni sia falsa, per rendere l'AND falso. Pertanto, non appena ne trova una falsa evita di valutare quelle successive e stabilisce che tutta la decisione è falsa.
</p>
<div class='vspace'></div><h4>MCDC</h4>
<p>Il test va preso in modo tale che ogni condizione all'interno di una decisione deve far variare in modo indipendente il valore finale nella decisione.
</p>
<p class='vspace'>Che diavolo vuol dire? Vuol dire che, se ho 4 condizioni combinate in vario modo, devo trovare dei valori tali per cui il valore di verità dipenda dal variare di una sola di esse.
</p>
<p class='vspace'>Ecco un esempio esplicativo: <strong>((((a OR b) AND c) OR d) AND e)</strong>.<br />Quando la <strong>a</strong>, e solo la <strong>a</strong>, è influente, ovvero, quand'è che il valore di verità della decisione dipende solo dalla variazione della <strong>a</strong>? La risposta è che <strong>a</strong> è influente quando:
</p><ul><li>b è falsa
</li><li>c è vera
</li><li>d è falsa
</li><li>e è vera
</li></ul><p>Perchè?
</p><ul><li>se b è falsa, allora <strong>a OR b</strong> dipende solo dalla a
</li><li>se c è vera, allora <strong>(a OR b) AND c</strong> dipende solo dal valore di verità di <strong>(a OR b)</strong>, che abbiamo visto dipendere solo da <strong>a</strong>
</li><li>se d è falsa, etc. etc.
</li></ul><p class='vspace'>Si fa lo stesso ragionamento per tutte le variabili, e si tirano fuori i casi di test relativi. Possono anche accadere dei casi in cui il valore di una variabile sia indifferente: lo si segna con un <strong>underscore</strong>: <strong>_</strong>.
</p>
<div class='vspace'></div><hr />
<p><a class='wikilink' href='LinguaggiProgrammazioneSicurezza.html'>Torna alla pagina di LPS</a>
</p>
</div>

						</td>
					</tr>
				</table>
			</td>
		</tr>
		<tr>
			<td id='foot' colspan='4' rowspan='1'>
				<table summary='Wiki: Footer' width='100%' cellpadding='0' cellspacing='0' border='0'>
				<!--PageFooterFmt-->
				<tr>
					<td id='footleft'>
					<span id='footchanges'>
						<a  rel='nofollow' href='RecentChanges.html'
						title='Ultime modifiche della sezione'
						accesskey='c'>Ultime modifiche</a>
						<a  rel='nofollow' href='http://www.swappa.it/wiki/Site.AllRecentChanges'
						title='(Tutte le modifiche del sito'
						accesskey='a'>(Tutte)</a>
					</span>
					<span id='footeditsb'>| 
						<a  rel='nofollow' href='http://www.swappa.it/wiki/Site.SideBar?action=edit'
						title='Modifica la barra laterale'
						accesskey='b'>edit SideBar</a>
					</span>
					</td>
					<td id='footmiddle'>
					<span id='footlastmod'>Ultimo aggiornamento: January 20, 2010, at 09:29 AM</span> 
					</td>
					<td id='footright'>
					<span id='footedit'>
						<a  rel='nofollow' href='http://www.swappa.it/wiki/Uni/LPSTesting?action=edit'
						title='Modifica la pagina'
						accesskey='e'>Modifica</a>
					</span>
					<span id='foothist'>| 
						<a  rel='nofollow' href='http://www.swappa.it/wiki/Uni/LPSTesting?action=diff'
						title='Ultime modifiche della pagina'
						accesskey='h'>Cronologia</a>
					</span> 
					</td>
				</tr>
				<!--/PageFooterFmt-->
				</table>
			</td>
		</tr>
	</tbody>
	</table>

	</body>
	
</html>
 
 
